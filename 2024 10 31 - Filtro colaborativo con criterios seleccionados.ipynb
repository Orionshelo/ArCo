{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###### ArCo ARMADO DE LA BASE#######\n",
    "import os\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "# Establecer el directorio de trabajo\n",
    "os.chdir(\"C:/Users/supegui/Documents/Metodología ArCo/Python/Excel\")\n",
    "# Saca la lista de archivos del directorio de trabajo\n",
    "#os.listdir()\n",
    "# Obtener la fecha actual\n",
    "fecha_actual = datetime.now().strftime(\"%Y %m %d\")\n",
    "#print(fecha_actual)\n",
    "\n",
    "# Construir el nombre del archivo basado en la fecha actual\n",
    "#nombre_archivo = f\"{fecha_actual} Informe ArCo.xlsx\"\n",
    "\n",
    "# Construir el nombre del archivo cuando la fecha no es la actual. Aquí solo se modifica la fecha. \n",
    "nombre_archivo = \"2024 10 30 - Base Original.xlsx\"\n",
    "\n",
    "data_arco = pd.read_excel(nombre_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Primero, creamos una matriz de características binarias para cada instrumento\n",
    "def crear_matriz_caracteristicas(data_arco):\n",
    "    # Seleccionamos las columnas relevantes\n",
    "    cols_usuarios = ['Emprendedores', 'Mipymes', 'Grandes empresas', 'Gobierno ', 'Academia', 'Entidades de soporte', 'Personas naturales']\n",
    "    \n",
    "    # Creamos una matriz donde cada fila es un instrumento y cada columna es un tipo de usuario\n",
    "    matriz_caracteristicas = data_arco[cols_usuarios].copy()\n",
    "    \n",
    "    # Convertimos 'Si'/'No' a 1/0\n",
    "    matriz_caracteristicas = (matriz_caracteristicas == 'Si').astype(int)\n",
    "    \n",
    "    return matriz_caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Emprendedores  Mipymes  Grandes empresas  Gobierno   Academia  \\\n",
      "0                0        0                 0          0         0   \n",
      "1                0        0                 1          0         0   \n",
      "2                0        1                 1          1         1   \n",
      "3                0        0                 0          1         0   \n",
      "4                0        0                 0          1         1   \n",
      "..             ...      ...               ...        ...       ...   \n",
      "413              0        0                 0          1         0   \n",
      "414              0        0                 0          0         0   \n",
      "415              0        1                 0          1         1   \n",
      "416              0        1                 0          1         1   \n",
      "417              0        0                 0          0         0   \n",
      "\n",
      "     Entidades de soporte  Personas naturales  \n",
      "0                       1                   0  \n",
      "1                       1                   0  \n",
      "2                       1                   1  \n",
      "3                       0                   0  \n",
      "4                       1                   1  \n",
      "..                    ...                 ...  \n",
      "413                     0                   0  \n",
      "414                     0                   1  \n",
      "415                     1                   1  \n",
      "416                     1                   1  \n",
      "417                     0                   1  \n",
      "\n",
      "[418 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "matriz_cara = crear_matriz_caracteristicas(data_arco)\n",
    "\n",
    "print(matriz_cara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# 2. Calculamos la similitud entre instrumentos - Jaccard\n",
    "def calcular_similitud(data_arco):\n",
    "    # Obtenemos la matriz de características\n",
    "    matriz = crear_matriz_caracteristicas(data_arco)\n",
    "    \n",
    "    # Inicializamos la matriz de similitud\n",
    "    n_instrumentos = len(data_arco)\n",
    "    matriz_similitud = np.zeros((n_instrumentos, n_instrumentos))\n",
    "    \n",
    "    # Calculamos la similitud para cada par de instrumentos\n",
    "    for i in range(n_instrumentos):\n",
    "        for j in range(n_instrumentos):\n",
    "            # Extraemos los vectores de características para los instrumentos i y j\n",
    "            instrumento_i = matriz.iloc[i].values\n",
    "            instrumento_j = matriz.iloc[j].values\n",
    "            \n",
    "            # Calculamos similitud usando coincidencias\n",
    "            coincidencias = sum(instrumento_i & instrumento_j)  # AND lógico\n",
    "            total_usuarios = sum(instrumento_i | instrumento_j)  # OR lógico\n",
    "            \n",
    "            # Calculamos el coeficiente de Jaccard\n",
    "            if total_usuarios > 0:\n",
    "                similitud = coincidencias / total_usuarios\n",
    "            else:\n",
    "                similitud = 0\n",
    "                \n",
    "            matriz_similitud[i][j] = similitud\n",
    "    \n",
    "    return matriz_similitud\n",
    "\"\"\"\"2024 10 25 - Base Original.xlsx\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calcular_similitud(data_arco):\n",
    "    # Obtenemos la matriz de características\n",
    "    matriz = crear_matriz_caracteristicas(data_arco)\n",
    "    \n",
    "    # Calculamos la matriz de similitud de coseno\n",
    "    matriz_similitud = cosine_similarity(matriz)\n",
    "    \n",
    "    return matriz_similitud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_jac = calcular_similitud(data_arco)\n",
    "#print(sim_jac)\n",
    "#sim_jac.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convertimos la similitud a ratings\n",
    "def calcular_ratings(data_arco):\n",
    "    # Calculamos la matriz de similitud\n",
    "    matriz_similitud = calcular_similitud(data_arco)\n",
    "    \n",
    "    # Creamos un DataFrame con los ratings\n",
    "    ratings = []\n",
    "    \n",
    "    for i in range(len(matriz_similitud)):\n",
    "        for j in range(len(matriz_similitud)):\n",
    "            if i != j:  # No incluimos la similitud de un instrumento consigo mismo\n",
    "                ratings.append({\n",
    "                    'Instrumento_A': data_arco.iloc[i]['Código'],\n",
    "                    'Nombre_A': data_arco.iloc[i]['Nombre del Instrumento'],\n",
    "                    'Instrumento_B': data_arco.iloc[j]['Código'],\n",
    "                    'Nombre_B': data_arco.iloc[j]['Nombre del Instrumento'],\n",
    "                    'Rating': round(matriz_similitud[i][j] * 5, 2)  # Convertimos a escala 0-5\n",
    "                })\n",
    "    \n",
    "    df_ratings = pd.DataFrame(ratings)\n",
    "    \n",
    "    # Ordenamos por rating de mayor a menor\n",
    "    df_ratings = df_ratings.sort_values('Rating', ascending=False)\n",
    "    \n",
    "    return df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Instrumento_A                                           Nombre_A  \\\n",
      "105855           2428  Línea Especial de Microcrédito Inclusión Finan...   \n",
      "122208           2800                CIBERPAZ EXPRESIÓN – LEGADO DE GABO   \n",
      "122180           2798                            Ciber Paz – Formaciones   \n",
      "122177           2798                            Ciber Paz – Formaciones   \n",
      "122175           2798                            Ciber Paz – Formaciones   \n",
      "122174           2798                            Ciber Paz – Formaciones   \n",
      "122173           2798                            Ciber Paz – Formaciones   \n",
      "122163           2798                            Ciber Paz – Formaciones   \n",
      "122143           2798                            Ciber Paz – Formaciones   \n",
      "122142           2798                            Ciber Paz – Formaciones   \n",
      "\n",
      "        Instrumento_B                                           Nombre_B  \\\n",
      "105855           3570  Beneficio de Capital Semilla para miembros act...   \n",
      "122208            438  Personas en reincorporación que acceden a la o...   \n",
      "122180           3711  Estrategia de Alimentos de Alto Valor Nutricional   \n",
      "122177           3693                               Educación Financiera   \n",
      "122175           3690  Fortalecimiento Estratégico a Emprendimientos ...   \n",
      "122174           3689                              Adecuación de tierras   \n",
      "122173           3684  Distritos Mineros Especiales para la Diversifi...   \n",
      "122163           3651  FORTALECIMIENTO DE LA GESTIÓN Y APROPIACIÓN DE...   \n",
      "122143           3618  Formación para estudios de maestría y doctorad...   \n",
      "122142           3617                Convocatoria Fullbright (2024-2025)   \n",
      "\n",
      "        Rating  \n",
      "105855     5.0  \n",
      "122208     5.0  \n",
      "122180     5.0  \n",
      "122177     5.0  \n",
      "122175     5.0  \n",
      "122174     5.0  \n",
      "122173     5.0  \n",
      "122163     5.0  \n",
      "122143     5.0  \n",
      "122142     5.0  \n"
     ]
    }
   ],
   "source": [
    "# 4. Función principal que integra todo el proceso\n",
    "def generar_ratings_colaborativos(data_arco):\n",
    "    # Calculamos los ratings\n",
    "    df_ratings = calcular_ratings(data_arco)\n",
    "    \n",
    "    # Filtramos solo los pares con rating significativo (por ejemplo, mayor a 2)\n",
    "    df_ratings_significativos = df_ratings[df_ratings['Rating'] > 2]\n",
    "    \n",
    "    return df_ratings_significativos\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ratings_finales = generar_ratings_colaborativos(data_arco)\n",
    "print(ratings_finales.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistema de recomendación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    # Crear y evaluar el sistema\\n    sistema = crear_y_evaluar_sistema(df_ratings)\\n    \\n    # Generar recomendaciones para un instrumento específico\\n    instrumento_ejemplo = df_ratings[\\'Instrumento_A\\'].iloc[126]\\n    recomendaciones = sistema.generar_recomendaciones(instrumento_ejemplo, n=20)\\n    print(\"\\nRecomendaciones para instrumento\", instrumento_ejemplo)\\n    print(recomendaciones)\\n    \\nexcept Exception as e:\\n    print(f\"Error: {str(e)}\")\\n    \\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Dataset, Reader, SVD, KNNBasic, NMF, accuracy, similarities\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from collections import defaultdict\n",
    "\n",
    "class SistemaRecomendacion:\n",
    "    def __init__(self, df_ratings):\n",
    "        \"\"\"\n",
    "        Inicializa el sistema de recomendación con el DataFrame de ratings\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df_ratings : pandas.DataFrame\n",
    "            DataFrame que debe contener las columnas ['Instrumento_A', 'Instrumento_B', 'Rating']\n",
    "        \"\"\"\n",
    "        # Validar que el DataFrame tiene las columnas necesarias\n",
    "        required_columns = ['Instrumento_A', 'Instrumento_B', 'Rating']\n",
    "        if not all(col in df_ratings.columns for col in required_columns):\n",
    "            raise ValueError(f\"El DataFrame debe contener las columnas: {required_columns}\")\n",
    "            \n",
    "        self.df_ratings = df_ratings.copy()\n",
    "        \n",
    "        # Verificar que no hay valores nulos\n",
    "        if self.df_ratings[required_columns].isna().any().any():\n",
    "            raise ValueError(\"El DataFrame contiene valores nulos\")\n",
    "        \n",
    "        # Aseguramos que los ratings están en el rango correcto\n",
    "        self.df_ratings['Rating'] = self.df_ratings['Rating'].clip(0, 5)\n",
    "        \n",
    "        # Configuramos el reader con un rango válido\n",
    "        self.reader = Reader(rating_scale=(0, 5))\n",
    "        \n",
    "        try:\n",
    "            # Convertimos a formato Surprise\n",
    "            self.data = Dataset.load_from_df(\n",
    "                self.df_ratings[required_columns], \n",
    "                self.reader\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error al convertir datos al formato Surprise: {str(e)}\")\n",
    "        \n",
    "        # Definimos los modelos base\n",
    "        self.modelos = {\n",
    "            'SVD': SVD(\n",
    "                n_factors=50,\n",
    "                n_epochs=20,\n",
    "                lr_all=0.005,\n",
    "                reg_all=0.02,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'NMF': NMF(\n",
    "                n_factors=15,\n",
    "                n_epochs=50,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Calcular el número de ratings por instrumento de manera correcta\n",
    "        ratings_a = self.df_ratings['Instrumento_A'].value_counts()\n",
    "        ratings_b = self.df_ratings['Instrumento_B'].value_counts()\n",
    "        \n",
    "        # Combinar los conteos\n",
    "        todos_instrumentos = set(ratings_a.index) | set(ratings_b.index)\n",
    "        ratings_totales = pd.Series(0, index=todos_instrumentos)\n",
    "        \n",
    "        for instrumento in todos_instrumentos:\n",
    "            total = (ratings_a.get(instrumento, 0) + \n",
    "                    ratings_b.get(instrumento, 0))\n",
    "            ratings_totales[instrumento] = total\n",
    "        \n",
    "        min_ratings = ratings_totales.min()\n",
    "        print(f\"Número mínimo de ratings por instrumento: {min_ratings}\")\n",
    "        print(f\"Número máximo de ratings por instrumento: {ratings_totales.max()}\")\n",
    "        print(f\"Promedio de ratings por instrumento: {ratings_totales.mean():.2f}\")\n",
    "        \n",
    "        # Agregamos KNN solo si tenemos suficientes datos y ratings\n",
    "        if len(self.df_ratings) > 10 and min_ratings >= 2:\n",
    "            # Calculamos k de manera más conservadora\n",
    "            k = min(\n",
    "                20,  # máximo valor de k\n",
    "                max(2, len(self.df_ratings) // 10)  # k proporcional al tamaño del dataset\n",
    "            )\n",
    "            \n",
    "            self.modelos['KNN'] = KNNBasic(\n",
    "                k=k,\n",
    "                min_k=2,\n",
    "                sim_options={\n",
    "                    'name': 'pearson_baseline',\n",
    "                    'user_based': False,\n",
    "                    'min_support': 2,\n",
    "                    'shrinkage': 100\n",
    "                }\n",
    "            )\n",
    "            print(f\"KNN configurado con k={k}\")\n",
    "        else:\n",
    "            print(\"No se incluirá KNN debido a insuficientes datos o ratings por instrumento\")\n",
    "        \n",
    "        self.mejor_modelo = None\n",
    "        self.metricas_modelos = {}\n",
    "        \n",
    "        print(f\"\\nTotal de instrumentos únicos: {len(todos_instrumentos)}\")\n",
    "        print(f\"Total de ratings: {len(self.df_ratings)}\")\n",
    "\n",
    "    def evaluar_modelos(self):\n",
    "        \"\"\"\n",
    "        Evalúa diferentes modelos usando validación cruzada\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Diccionario con las métricas de evaluación de cada modelo\n",
    "        \"\"\"\n",
    "        if len(self.df_ratings) < 5:\n",
    "            raise ValueError(\"Se necesitan al menos 5 ratings para evaluar los modelos\")\n",
    "            \n",
    "        print(\"\\nEvaluando modelos...\")\n",
    "        for nombre, modelo in self.modelos.items():\n",
    "            try:\n",
    "                # Realizar validación cruzada\n",
    "                n_folds = min(5, len(self.df_ratings) // 2)\n",
    "                n_folds = max(2, n_folds)\n",
    "                \n",
    "                resultados = cross_validate(\n",
    "                    modelo, \n",
    "                    self.data, \n",
    "                    measures=['RMSE', 'MAE'], \n",
    "                    cv=n_folds, \n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                self.metricas_modelos[nombre] = {\n",
    "                    'RMSE_medio': np.mean(resultados['test_rmse']),\n",
    "                    'MAE_medio': np.mean(resultados['test_mae']),\n",
    "                    'tiempo_fit_medio': np.mean(resultados['fit_time']),\n",
    "                    'tiempo_test_medio': np.mean(resultados['test_time'])\n",
    "                }\n",
    "                \n",
    "                print(f\"✓ Modelo {nombre} evaluado exitosamente\")\n",
    "                print(f\"  RMSE: {self.metricas_modelos[nombre]['RMSE_medio']:.4f}\")\n",
    "                print(f\"  MAE: {self.metricas_modelos[nombre]['MAE_medio']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error evaluando modelo {nombre}: {str(e)}\")\n",
    "                print(\"  Este modelo será excluido de las recomendaciones\")\n",
    "                continue\n",
    "        \n",
    "        if not self.metricas_modelos:\n",
    "            raise ValueError(\"Ningún modelo pudo ser evaluado exitosamente\")\n",
    "            \n",
    "        # Encontrar el mejor modelo basado en RMSE\n",
    "        mejor_modelo_nombre = min(\n",
    "            self.metricas_modelos, \n",
    "            key=lambda x: self.metricas_modelos[x]['RMSE_medio']\n",
    "        )\n",
    "        self.mejor_modelo = self.modelos[mejor_modelo_nombre]\n",
    "        print(f\"\\n→ Mejor modelo: {mejor_modelo_nombre}\")\n",
    "        \n",
    "        return self.metricas_modelos\n",
    "\n",
    "    def evaluar_modelos(self):\n",
    "        \"\"\"\n",
    "        Evalúa diferentes modelos usando validación cruzada\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Diccionario con las métricas de evaluación de cada modelo\n",
    "        \"\"\"\n",
    "        if len(self.df_ratings) < 5:\n",
    "            raise ValueError(\"Se necesitan al menos 5 ratings para evaluar los modelos\")\n",
    "            \n",
    "        print(\"\\nEvaluando modelos...\")\n",
    "        for nombre, modelo in self.modelos.items():\n",
    "            try:\n",
    "                # Realizar validación cruzada\n",
    "                n_folds = min(5, len(self.df_ratings) // 2)\n",
    "                n_folds = max(2, n_folds)\n",
    "                \n",
    "                resultados = cross_validate(\n",
    "                    modelo, \n",
    "                    self.data, \n",
    "                    measures=['RMSE', 'MAE'], \n",
    "                    cv=n_folds, \n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                self.metricas_modelos[nombre] = {\n",
    "                    'RMSE_medio': np.mean(resultados['test_rmse']),\n",
    "                    'MAE_medio': np.mean(resultados['test_mae']),\n",
    "                    'tiempo_fit_medio': np.mean(resultados['fit_time']),\n",
    "                    'tiempo_test_medio': np.mean(resultados['test_time'])\n",
    "                }\n",
    "                \n",
    "                print(f\"✓ Modelo {nombre} evaluado exitosamente\")\n",
    "                print(f\"  RMSE: {self.metricas_modelos[nombre]['RMSE_medio']:.4f}\")\n",
    "                print(f\"  MAE: {self.metricas_modelos[nombre]['MAE_medio']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error evaluando modelo {nombre}: {str(e)}\")\n",
    "                print(\"  Este modelo será excluido de las recomendaciones\")\n",
    "                continue\n",
    "        \n",
    "        if not self.metricas_modelos:\n",
    "            raise ValueError(\"Ningún modelo pudo ser evaluado exitosamente\")\n",
    "            \n",
    "        # Encontrar el mejor modelo basado en RMSE\n",
    "        mejor_modelo_nombre = min(\n",
    "            self.metricas_modelos, \n",
    "            key=lambda x: self.metricas_modelos[x]['RMSE_medio']\n",
    "        )\n",
    "        self.mejor_modelo = self.modelos[mejor_modelo_nombre]\n",
    "        print(f\"\\n→ Mejor modelo: {mejor_modelo_nombre}\")\n",
    "        \n",
    "        return self.metricas_modelos\n",
    "\n",
    "    def entrenar_mejor_modelo(self):\n",
    "        \"\"\"\n",
    "        Entrena el mejor modelo con todos los datos\n",
    "        \"\"\"\n",
    "        if self.mejor_modelo is None:\n",
    "            raise ValueError(\"Debe evaluar los modelos primero\")\n",
    "            \n",
    "        trainset = self.data.build_full_trainset()\n",
    "        self.mejor_modelo.fit(trainset)\n",
    "\n",
    "    def generar_recomendaciones(self, instrumento_id, n=5):\n",
    "        \"\"\"\n",
    "        Genera recomendaciones para un instrumento específico con manejo de errores\n",
    "        \"\"\"\n",
    "        if self.mejor_modelo is None:\n",
    "            raise ValueError(\"Debe entrenar el modelo primero\")\n",
    "            \n",
    "        # Obtener todos los instrumentos únicos\n",
    "        instrumentos_unicos = set(self.df_ratings['Instrumento_A'].unique()) | \\\n",
    "                            set(self.df_ratings['Instrumento_B'].unique())\n",
    "        \n",
    "        if instrumento_id not in instrumentos_unicos:\n",
    "            raise ValueError(f\"Instrumento {instrumento_id} no encontrado en el conjunto de datos\")\n",
    "        \n",
    "        # Generar predicciones para todos los pares posibles\n",
    "        predicciones = []\n",
    "        for otro_instrumento in instrumentos_unicos:\n",
    "            if otro_instrumento != instrumento_id:\n",
    "                try:\n",
    "                    pred = self.mejor_modelo.predict(instrumento_id, otro_instrumento)\n",
    "                    predicciones.append({\n",
    "                        'Instrumento_origen': instrumento_id,\n",
    "                        'Instrumento_recomendado': otro_instrumento,\n",
    "                        'Rating_predicho': pred.est,\n",
    "                        'Detalles': pred.details\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error prediciendo para {otro_instrumento}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not predicciones:\n",
    "            raise ValueError(\"No se pudieron generar predicciones\")\n",
    "            \n",
    "        # Convertir a DataFrame y ordenar\n",
    "        df_recomendaciones = pd.DataFrame(predicciones)\n",
    "        df_recomendaciones = df_recomendaciones.sort_values(\n",
    "            'Rating_predicho', \n",
    "            ascending=False\n",
    "        ).head(n)\n",
    "        \n",
    "        return df_recomendaciones\n",
    "\n",
    "    def mostrar_metricas(self):\n",
    "        \"\"\"\n",
    "        Muestra las métricas de evaluación de todos los modelos\n",
    "        \"\"\"\n",
    "        if not self.metricas_modelos:\n",
    "            raise ValueError(\"No hay métricas disponibles. Debe evaluar los modelos primero\")\n",
    "            \n",
    "        df_metricas = pd.DataFrame(self.metricas_modelos).round(4)\n",
    "        return df_metricas.transpose()\n",
    "\n",
    "def crear_y_evaluar_sistema(df_ratings):\n",
    "    \"\"\"\n",
    "    Función principal para crear y evaluar el sistema de recomendación\n",
    "    \"\"\"\n",
    "    # Verificar que tenemos suficientes datos\n",
    "    if len(df_ratings) < 5:\n",
    "        raise ValueError(\"Se necesitan al menos 5 ratings para crear el sistema\")\n",
    "        \n",
    "    # Verificar que tenemos variabilidad en los ratings\n",
    "    if df_ratings['Rating'].std() < 0.1:\n",
    "        raise ValueError(\"No hay suficiente variabilidad en los ratings\")\n",
    "        \n",
    "    # Crear sistema\n",
    "    sistema = SistemaRecomendacion(df_ratings)\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    try:\n",
    "        metricas = sistema.evaluar_modelos()\n",
    "        print(\"\\nMétricas de evaluación:\")\n",
    "        print(sistema.mostrar_metricas())\n",
    "        \n",
    "        # Entrenar mejor modelo\n",
    "        sistema.entrenar_mejor_modelo()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la evaluación del sistema: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return sistema\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\"\"\"\n",
    "try:\n",
    "    # Crear y evaluar el sistema\n",
    "    sistema = crear_y_evaluar_sistema(df_ratings)\n",
    "    \n",
    "    # Generar recomendaciones para un instrumento específico\n",
    "    instrumento_ejemplo = df_ratings['Instrumento_A'].iloc[126]\n",
    "    recomendaciones = sistema.generar_recomendaciones(instrumento_ejemplo, n=20)\n",
    "    print(\"\\nRecomendaciones para instrumento\", instrumento_ejemplo)\n",
    "    print(recomendaciones)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número mínimo de ratings por instrumento: 834\n",
      "Número máximo de ratings por instrumento: 834\n",
      "Promedio de ratings por instrumento: 834.00\n",
      "KNN configurado con k=20\n",
      "\n",
      "Total de instrumentos únicos: 418\n",
      "Total de ratings: 174306\n",
      "\n",
      "Evaluando modelos...\n",
      "✓ Modelo SVD evaluado exitosamente\n",
      "  RMSE: 0.1060\n",
      "  MAE: 0.0644\n",
      "✓ Modelo NMF evaluado exitosamente\n",
      "  RMSE: 0.2502\n",
      "  MAE: 0.1733\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "✓ Modelo KNN evaluado exitosamente\n",
      "  RMSE: 0.3794\n",
      "  MAE: 0.1941\n",
      "\n",
      "→ Mejor modelo: SVD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SVD': {'RMSE_medio': 0.10600988073888921,\n",
       "  'MAE_medio': 0.06436936303492209,\n",
       "  'tiempo_fit_medio': 0.7374232769012451,\n",
       "  'tiempo_test_medio': 0.13460288047790528},\n",
       " 'NMF': {'RMSE_medio': 0.2502034455871976,\n",
       "  'MAE_medio': 0.17329091713421355,\n",
       "  'tiempo_fit_medio': 1.5844726085662841,\n",
       "  'tiempo_test_medio': 0.1448535442352295},\n",
       " 'KNN': {'RMSE_medio': 0.3794134750392395,\n",
       "  'MAE_medio': 0.19411382543610706,\n",
       "  'tiempo_fit_medio': 2.408037233352661,\n",
       "  'tiempo_test_medio': 4.410260152816773}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear y entrenar el sistema de recomendación\n",
    "sistema = SistemaRecomendacion(df_ratings)\n",
    "\n",
    "sistema.evaluar_modelos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sistema.entrenar_mejor_modelo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio)\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.19.3 (from gradio)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (3.9.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.10-cp39-none-win_amd64.whl.metadata (51 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.17-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.7.1-py3-none-win_amd64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fsspec (from gradio-client==1.3.0->gradio)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
      "  Downloading websockets-12.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1.0->gradio)\n",
      "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.19.3->gradio)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.19.3->gradio)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.20.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.23.4-cp39-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\supegui\\appdata\\local\\anaconda3\\envs\\surprise_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "   ---------------------------------------- 0.0/18.1 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 6.3/18.1 MB 32.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.9/18.1 MB 37.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.1/18.1 MB 36.7 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Downloading orjson-3.10.10-cp39-none-win_amd64.whl (139 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp39-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 21.3 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
      "Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Downloading ruff-0.7.1-py3-none-win_amd64.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.9/9.4 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.8/9.4 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.4 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
      "Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Downloading websockets-12.0-cp39-cp39-win_amd64.whl (124 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: pydub, websockets, tqdm, tomlkit, sniffio, shellingham, semantic-version, ruff, pyyaml, python-multipart, pydantic-core, orjson, markupsafe, h11, fsspec, filelock, ffmpy, annotated-types, aiofiles, uvicorn, pydantic, huggingface-hub, httpcore, anyio, typer, starlette, httpx, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 anyio-4.6.2.post1 fastapi-0.115.4 ffmpy-0.4.0 filelock-3.16.1 fsspec-2024.10.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.2 markupsafe-2.1.5 orjson-3.10.10 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 python-multipart-0.0.17 pyyaml-6.0.2 ruff-0.7.1 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.41.2 tomlkit-0.12.0 tqdm-4.66.6 typer-0.12.5 uvicorn-0.32.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "# Supón que ya tienes el DataFrame `df_ratings` cargado\n",
    "# y los diccionarios `nombre_a_codigo` y `nombre_a_descripcion` configurados.\n",
    "\n",
    "# Crear y entrenar el sistema de recomendación\n",
    "#sistema = SistemaRecomendacion(df_ratings)\n",
    "#sistema.evaluar_modelos()\n",
    "#sistema.entrenar_mejor_modelo()\n",
    "\n",
    "def obtener_recomendaciones_graficas(instrumento_nombre, n=5):\n",
    "    \"\"\"\n",
    "    Genera las recomendaciones para un instrumento específico en la interfaz gráfica\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertimos el nombre del instrumento al código correspondiente\n",
    "        instrumento_id = nombre_a_codigo[instrumento_nombre]\n",
    "\n",
    "        # Generamos recomendaciones\n",
    "        recomendaciones = sistema.generar_recomendaciones(instrumento_id, n)\n",
    "\n",
    "        # Construimos el resultado en formato de texto para mostrar en Gradio\n",
    "        resultado = f\"Recomendaciones para el instrumento: {instrumento_nombre}\\n\\n\"\n",
    "        for i, row in recomendaciones.iterrows():\n",
    "            nombre_recomendado = nombre_a_descripcion.get(row['Instrumento_recomendado'], \"Desconocido\")\n",
    "            resultado += f\"Instrumento recomendado: {nombre_recomendado}\\n\"\n",
    "            resultado += f\"Rating predicho: {row['Rating_predicho']:.2f}\\n\\n\"\n",
    "\n",
    "        return resultado\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error generando recomendaciones: {str(e)}\"\n",
    "\n",
    "# Configurar nombres de instrumentos en un dropdown para el selector\n",
    "nombres_instrumentos = sorted(df_ratings['Nombre_B'].unique())\n",
    "\n",
    "# Crear la interfaz de Gradio\n",
    "demo = gr.Interface(\n",
    "    fn=obtener_recomendaciones_graficas,\n",
    "    inputs=gr.Dropdown(\n",
    "        choices=nombres_instrumentos,\n",
    "        label=\"Seleccione un instrumento por nombre\"\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        label=\"Recomendaciones\",\n",
    "        lines=10\n",
    "    ),\n",
    "    title=\"Sistema de Recomendación de Instrumentos\",\n",
    "    description=\"Seleccione un instrumento para ver recomendaciones de otros instrumentos similares con el mejor rating predicho.\",\n",
    "    theme=\"default\"\n",
    ")\n",
    "\n",
    "# Iniciar la aplicación\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surprise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
